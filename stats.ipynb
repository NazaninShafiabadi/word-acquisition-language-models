{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import string\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import spacy\n",
    "import codecs\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_tags(doc_path):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    nlp.max_length = 2000000\n",
    "    pos_dict = {}\n",
    "    with open(doc_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        doc = nlp(text)\n",
    "        for token in doc:\n",
    "            if token.text in pos_dict and not token.pos_ in pos_dict[token.text]:\n",
    "                pos_dict[token.text].append(token.pos_)\n",
    "            else:\n",
    "                pos_dict[token.text] = [token.pos_]\n",
    "        \n",
    "    return pd.DataFrame(list(pos_dict.items()), columns=['token', 'POS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating wordbank file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'isascii'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-91025a7ae47c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'count'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mw\u001b[0m  \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/internship_2024/venv/lib64/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4211\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4212\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4213\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4215\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-91025a7ae47c>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(w)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'count'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mw\u001b[0m  \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'isascii'"
     ]
    }
   ],
   "source": [
    "with open('sample_data/wikitext/wikitext103_test.txt', 'r') as file:\n",
    "    contents = file.read()\n",
    "\n",
    "words = contents.split()\n",
    "df = pd.DataFrame(list(Counter(words).items()), columns=['token', 'count']).sort_values('count', ascending=False).reset_index(drop=True)\n",
    "df = df[df['token'].apply(lambda w: w.isascii() and w  not in string.punctuation and not w.isdigit())]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.8 (default, Apr 16 2020, 01:36:27) \n",
      "[GCC 8.3.1 20191121 (Red Hat 8.3.1-5)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of words that are considered one token by the language model: 67.64%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>13988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>of</td>\n",
       "      <td>6731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and</td>\n",
       "      <td>5780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>to</td>\n",
       "      <td>4724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>in</td>\n",
       "      <td>4495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13626</th>\n",
       "      <td>Silence</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13627</th>\n",
       "      <td>hid</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13628</th>\n",
       "      <td>eyebrows</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13629</th>\n",
       "      <td>Straps</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13630</th>\n",
       "      <td>jade</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13631 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          token  count\n",
       "0           the  13988\n",
       "1            of   6731\n",
       "2           and   5780\n",
       "3            to   4724\n",
       "4            in   4495\n",
       "...         ...    ...\n",
       "13626   Silence      1\n",
       "13627       hid      1\n",
       "13628  eyebrows      1\n",
       "13629    Straps      1\n",
       "13630      jade      1\n",
       "\n",
       "[13631 rows x 2 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only keeping words that are considered one token by the language model\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/multiberts-seed_0')\n",
    "filtered_words = [word for word in df['token'].tolist() if len(tokenizer.tokenize(word)) == 1]\n",
    "filtered_df = df[df['token'].isin(filtered_words)].reset_index(drop=True)\n",
    "\n",
    "print(f'Percentage of words that are considered one token by the language model: {((len(filtered_df) / len(df)) * 100):.2f}%')\n",
    "\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>[SPACE]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Robert</td>\n",
       "      <td>[PROPN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Boulter</td>\n",
       "      <td>[PROPN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>is</td>\n",
       "      <td>[AUX]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>an</td>\n",
       "      <td>[DET]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20889</th>\n",
       "      <td>Author</td>\n",
       "      <td>[NOUN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20890</th>\n",
       "      <td>credibility</td>\n",
       "      <td>[NOUN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20891</th>\n",
       "      <td>Ronnie</td>\n",
       "      <td>[PROPN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20892</th>\n",
       "      <td>Pelkey</td>\n",
       "      <td>[PROPN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20893</th>\n",
       "      <td>\\n\\n</td>\n",
       "      <td>[SPACE]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20894 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             token      POS\n",
       "0                   [SPACE]\n",
       "1           Robert  [PROPN]\n",
       "2          Boulter  [PROPN]\n",
       "3               is    [AUX]\n",
       "4               an    [DET]\n",
       "...            ...      ...\n",
       "20889       Author   [NOUN]\n",
       "20890  credibility   [NOUN]\n",
       "20891       Ronnie  [PROPN]\n",
       "20892       Pelkey  [PROPN]\n",
       "20893         \\n\\n  [SPACE]\n",
       "\n",
       "[20894 rows x 2 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = \"sample_data/wikitext/wikitext103_test.txt\"\n",
    "pos_tags = get_pos_tags(document)\n",
    "pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>count</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "      <td>[NUM]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>00</td>\n",
       "      <td>31</td>\n",
       "      <td>[PUNCT]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>000</td>\n",
       "      <td>186</td>\n",
       "      <td>[NUM]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11241</th>\n",
       "      <td>001</td>\n",
       "      <td>1</td>\n",
       "      <td>[NUM]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5468</th>\n",
       "      <td>01</td>\n",
       "      <td>4</td>\n",
       "      <td>[NUM]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7160</th>\n",
       "      <td>→</td>\n",
       "      <td>2</td>\n",
       "      <td>[ADP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4561</th>\n",
       "      <td>−</td>\n",
       "      <td>5</td>\n",
       "      <td>[PROPN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11175</th>\n",
       "      <td>♯</td>\n",
       "      <td>1</td>\n",
       "      <td>[PROPN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13116</th>\n",
       "      <td>東</td>\n",
       "      <td>1</td>\n",
       "      <td>[PROPN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10044</th>\n",
       "      <td>王</td>\n",
       "      <td>1</td>\n",
       "      <td>[VERB]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14195 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      token  count      POS\n",
       "237       0     84    [NUM]\n",
       "823      00     31  [PUNCT]\n",
       "79      000    186    [NUM]\n",
       "11241   001      1    [NUM]\n",
       "5468     01      4    [NUM]\n",
       "...     ...    ...      ...\n",
       "7160      →      2    [ADP]\n",
       "4561      −      5  [PROPN]\n",
       "11175     ♯      1  [PROPN]\n",
       "13116     東      1  [PROPN]\n",
       "10044     王      1   [VERB]\n",
       "\n",
       "[14195 rows x 3 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.sort_values('token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>count</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>13988</td>\n",
       "      <td>[DET]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>of</td>\n",
       "      <td>6731</td>\n",
       "      <td>[ADP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and</td>\n",
       "      <td>5780</td>\n",
       "      <td>[CCONJ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>to</td>\n",
       "      <td>4724</td>\n",
       "      <td>[ADP, PART]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>in</td>\n",
       "      <td>4495</td>\n",
       "      <td>[ADP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14190</th>\n",
       "      <td>Silence</td>\n",
       "      <td>1</td>\n",
       "      <td>[PROPN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14191</th>\n",
       "      <td>hid</td>\n",
       "      <td>1</td>\n",
       "      <td>[VERB]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14192</th>\n",
       "      <td>eyebrows</td>\n",
       "      <td>1</td>\n",
       "      <td>[NOUN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14193</th>\n",
       "      <td>Straps</td>\n",
       "      <td>1</td>\n",
       "      <td>[NOUN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14194</th>\n",
       "      <td>jade</td>\n",
       "      <td>1</td>\n",
       "      <td>[NOUN]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14195 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          token  count          POS\n",
       "0           the  13988        [DET]\n",
       "1            of   6731        [ADP]\n",
       "2           and   5780      [CCONJ]\n",
       "3            to   4724  [ADP, PART]\n",
       "4            in   4495        [ADP]\n",
       "...         ...    ...          ...\n",
       "14190   Silence      1      [PROPN]\n",
       "14191       hid      1       [VERB]\n",
       "14192  eyebrows      1       [NOUN]\n",
       "14193    Straps      1       [NOUN]\n",
       "14194      jade      1       [NOUN]\n",
       "\n",
       "[14195 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = pd.merge(filtered_df, pos_tags, on='token', how='inner')\n",
    "merged_df.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('sample_data/wikitext/wikitext_wordbank.tsv', sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_sentences(tokenizer, wordbank_file, tokenized_examples_file,\n",
    "                         max_seq_len, min_seq_len, max_samples, bidirectional=True):\n",
    "    short_sents = 0\n",
    "    long_sents = 0\n",
    "    superfluous = 0\n",
    "    num_lines = 0\n",
    "    # Each entry of token data is a tuple of token, token_id, masked_sample_sentences.\n",
    "    token_data = []\n",
    "    # Load words.\n",
    "    df = pd.read_csv(wordbank_file, sep='\\t')\n",
    "    wordbank_tokens = df.token.unique().tolist()\n",
    "    # Get token ids.\n",
    "    for token in wordbank_tokens:\n",
    "        token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "        if token_id != tokenizer.unk_token_id:\n",
    "            token_data.append(tuple([token, token_id, []]))\n",
    "    # Load sentences.\n",
    "    print(f\"Loading sentences from {tokenized_examples_file}.\")\n",
    "    infile = codecs.open(tokenized_examples_file, 'rb', encoding='utf-8')\n",
    "    for line_count, line in enumerate(infile):\n",
    "        num_lines += 1\n",
    "        if line_count % 100000 == 0:\n",
    "            print(\"Finished line {}.\".format(line_count))\n",
    "        example_string = line.strip()\n",
    "        example = [int(token_id) for token_id in example_string.split()]\n",
    "        # Use the pair of sentences (instead of individual sentences), to have\n",
    "        # longer sequences. Also more similar to training.\n",
    "        if len(example) < min_seq_len:\n",
    "            short_sents += 1\n",
    "            continue\n",
    "        if len(example) > max_seq_len:\n",
    "            long_sents += 1\n",
    "            example = example[:max_seq_len]\n",
    "        for token, token_id, sample_sents in token_data:\n",
    "            if len(sample_sents) >= max_samples:\n",
    "                # This token already has enough sentences.\n",
    "                superfluous += 1\n",
    "                continue\n",
    "            token_indices = [index for index, curr_id in enumerate(example) if curr_id == token_id]\n",
    "            # Warning: in bidirectional contexts, the mask can be in the first or last position,\n",
    "            # which can cause no mask prediction to be made for the biLSTM.\n",
    "            if not bidirectional:\n",
    "                # The token must have enough unidirectional context.\n",
    "                # The sequence length (including the target token) must be at least min_seq_len.\n",
    "                token_indices = [index for index in token_indices if index >= min_seq_len-1]\n",
    "            if len(token_indices) > 0:\n",
    "                new_example = example.copy()\n",
    "                mask_idx = random.choice(token_indices)\n",
    "                new_example[mask_idx] = tokenizer.mask_token_id\n",
    "                sample_sents.append(new_example)\n",
    "    infile.close()\n",
    "    # Logging.\n",
    "    print(f'{superfluous} out of {len(token_data)} tokens ({(superfluous/len(token_data))*100:.2f}%) had more than {max_samples} samples.')\n",
    "    print(f'{((len(token_data) - superfluous)/len(token_data))*100:.2f}% of tokens had a maximum of {max_samples} samples.')\n",
    "    print(f'{short_sents} examples were shorter than {min_seq_len} tokens and were thus disregarded. {((num_lines-short_sents)/num_lines)*100:.2f}% of examples were kept.')\n",
    "    print(f'{long_sents} examples were longer than {max_seq_len} tokens and were thus clipped.')\n",
    "    return token_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sentences from sample_data/wikitext/test_tokenized.txt.\n",
      "Finished line 0.\n",
      "4841 out of 9886 tokens (48.97%) had more than 512 samples.\n",
      "51.03% of tokens had a maximum of 512 samples.\n",
      "6 examples were shorter than 8 tokens and were thus disregarded. 99.45% of examples were kept.\n",
      "60 examples were longer than 512 tokens and were thus clipped.\n"
     ]
    }
   ],
   "source": [
    "wordbank_file = 'sample_data/wikitext/wikitext_wordbank.tsv'\n",
    "tokenized_examples_file = 'sample_data/wikitext/test_tokenized.txt'\n",
    "config = AutoConfig.from_pretrained('google/multiberts-seed_0')\n",
    "max_seq_len = config.max_position_embeddings\n",
    "min_seq_len = 8\n",
    "max_samples = 512\n",
    "\n",
    "token_data = get_sample_sentences(\n",
    "    tokenizer, wordbank_file, tokenized_examples_file, max_seq_len, min_seq_len, max_samples\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9886"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>token_id</th>\n",
       "      <th>sample_sents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>1996</td>\n",
       "      <td>[[101, 2728, 8945, 11314, 2121, 2003, 2019, 23...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>of</td>\n",
       "      <td>1997</td>\n",
       "      <td>[[101, 2728, 8945, 11314, 2121, 2003, 2019, 23...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and</td>\n",
       "      <td>1998</td>\n",
       "      <td>[[101, 2728, 8945, 11314, 2121, 2003, 2019, 23...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>to</td>\n",
       "      <td>2000</td>\n",
       "      <td>[[101, 2728, 8945, 11314, 2121, 2003, 2019, 23...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>in</td>\n",
       "      <td>1999</td>\n",
       "      <td>[[101, 2728, 8945, 11314, 2121, 2003, 2019, 23...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9881</th>\n",
       "      <td>investigators</td>\n",
       "      <td>14766</td>\n",
       "      <td>[[101, 2035, 9171, 1997, 1996, 4484, 4641, 199...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9882</th>\n",
       "      <td>clawed</td>\n",
       "      <td>22544</td>\n",
       "      <td>[[101, 2474, 18834, 2050, 6104, 1017, 4832, 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9883</th>\n",
       "      <td>hid</td>\n",
       "      <td>11041</td>\n",
       "      <td>[[101, 12500, 8042, 1516, 2085, 3938, 2086, 22...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9884</th>\n",
       "      <td>eyebrows</td>\n",
       "      <td>8407</td>\n",
       "      <td>[[101, 2474, 18834, 2050, 6104, 1017, 4832, 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9885</th>\n",
       "      <td>jade</td>\n",
       "      <td>12323</td>\n",
       "      <td>[[101, 2474, 18834, 2050, 6104, 1017, 4832, 10...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9886 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              token  token_id  \\\n",
       "0               the      1996   \n",
       "1                of      1997   \n",
       "2               and      1998   \n",
       "3                to      2000   \n",
       "4                in      1999   \n",
       "...             ...       ...   \n",
       "9881  investigators     14766   \n",
       "9882         clawed     22544   \n",
       "9883            hid     11041   \n",
       "9884       eyebrows      8407   \n",
       "9885           jade     12323   \n",
       "\n",
       "                                           sample_sents  \n",
       "0     [[101, 2728, 8945, 11314, 2121, 2003, 2019, 23...  \n",
       "1     [[101, 2728, 8945, 11314, 2121, 2003, 2019, 23...  \n",
       "2     [[101, 2728, 8945, 11314, 2121, 2003, 2019, 23...  \n",
       "3     [[101, 2728, 8945, 11314, 2121, 2003, 2019, 23...  \n",
       "4     [[101, 2728, 8945, 11314, 2121, 2003, 2019, 23...  \n",
       "...                                                 ...  \n",
       "9881  [[101, 2035, 9171, 1997, 1996, 4484, 4641, 199...  \n",
       "9882  [[101, 2474, 18834, 2050, 6104, 1017, 4832, 10...  \n",
       "9883  [[101, 12500, 8042, 1516, 2085, 3938, 2086, 22...  \n",
       "9884  [[101, 2474, 18834, 2050, 6104, 1017, 4832, 10...  \n",
       "9885  [[101, 2474, 18834, 2050, 6104, 1017, 4832, 10...  \n",
       "\n",
       "[9886 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('sample_data/wikitext/bidirectional_samples.pickle', 'rb') as f:\n",
    "    bidirectional_samples = pd.DataFrame(pickle.load(f), columns=['token', 'token_id', 'sample_sents'])\n",
    "\n",
    "bidirectional_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>token_id</th>\n",
       "      <th>sample_sents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>1996</td>\n",
       "      <td>[[101, 2728, 8945, 11314, 2121, 2003, 2019, 23...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>of</td>\n",
       "      <td>1997</td>\n",
       "      <td>[[101, 2728, 8945, 11314, 2121, 2003, 2019, 23...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and</td>\n",
       "      <td>1998</td>\n",
       "      <td>[[101, 2728, 8945, 11314, 2121, 2003, 2019, 23...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>to</td>\n",
       "      <td>2000</td>\n",
       "      <td>[[101, 2728, 8945, 11314, 2121, 2003, 2019, 23...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>in</td>\n",
       "      <td>1999</td>\n",
       "      <td>[[101, 2728, 8945, 11314, 2121, 2003, 2019, 23...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9559</th>\n",
       "      <td>warrior</td>\n",
       "      <td>6750</td>\n",
       "      <td>[[101, 1043, 4135, 7442, 1005, 1055, 4799, 673...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9574</th>\n",
       "      <td>i</td>\n",
       "      <td>1045</td>\n",
       "      <td>[[101, 1999, 2294, 8945, 11314, 2121, 5652, 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9671</th>\n",
       "      <td>ve</td>\n",
       "      <td>2310</td>\n",
       "      <td>[[101, 1996, 10860, 2189, 2678, 1010, 2856, 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9709</th>\n",
       "      <td>summers</td>\n",
       "      <td>10945</td>\n",
       "      <td>[[101, 4649, 11802, 6006, 2006, 16779, 2547, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9773</th>\n",
       "      <td>earth</td>\n",
       "      <td>3011</td>\n",
       "      <td>[[101, 2002, 2094, 21877, 1010, 2036, 2124, 20...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2557 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        token  token_id                                       sample_sents\n",
       "0         the      1996  [[101, 2728, 8945, 11314, 2121, 2003, 2019, 23...\n",
       "1          of      1997  [[101, 2728, 8945, 11314, 2121, 2003, 2019, 23...\n",
       "2         and      1998  [[101, 2728, 8945, 11314, 2121, 2003, 2019, 23...\n",
       "3          to      2000  [[101, 2728, 8945, 11314, 2121, 2003, 2019, 23...\n",
       "4          in      1999  [[101, 2728, 8945, 11314, 2121, 2003, 2019, 23...\n",
       "...       ...       ...                                                ...\n",
       "9559  warrior      6750  [[101, 1043, 4135, 7442, 1005, 1055, 4799, 673...\n",
       "9574        i      1045  [[101, 1999, 2294, 8945, 11314, 2121, 5652, 19...\n",
       "9671       ve      2310  [[101, 1996, 10860, 2189, 2678, 1010, 2856, 20...\n",
       "9709  summers     10945  [[101, 4649, 11802, 6006, 2006, 16779, 2547, 2...\n",
       "9773    earth      3011  [[101, 2002, 2094, 21877, 1010, 2036, 2124, 20...\n",
       "\n",
       "[2557 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final = bidirectional_samples[bidirectional_samples['sample_sents'].apply(lambda x: len(x) >= 8)]\n",
    "final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
